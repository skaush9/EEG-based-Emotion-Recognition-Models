{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e30a8c-e5d3-4beb-8602-b80746dd7dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "\n",
    "import random\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffa8b63-0856-4d57-b32c-49ecaa54436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 examples of training data:\n",
      "[[26.94933313 26.94942352 26.86116345 ... 16.22032493 17.53295088\n",
      "  17.53075379]\n",
      " [26.95540644 26.95561349 26.86818211 ... 16.22077216 17.53405894\n",
      "  17.53186146]\n",
      " [26.96394591 26.96354065 26.87468671 ... 16.22162589 17.53570343\n",
      "  17.53347345]\n",
      " ...\n",
      " [26.95091461 26.95146416 26.85930528 ... 16.21242109 17.53291899\n",
      "  17.53075099]\n",
      " [26.93989902 26.94184343 26.84918537 ... 16.21115102 17.5320002\n",
      "  17.52995518]\n",
      " [26.92817907 26.9306008  26.83702679 ... 16.21079389 17.53139138\n",
      "  17.52946743]]\n",
      "\n",
      "Corresponding labels for the training data:\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "First 10 examples of test data:\n",
      "[[26.88684078 26.36006069 26.2375236  ... 16.32757215 16.90493887\n",
      "  16.7821357 ]\n",
      " [26.88612478 26.35698842 26.23444401 ... 16.33011158 16.90430262\n",
      "  16.78066285]\n",
      " [26.88125864 26.3487917  26.22622124 ... 16.33296166 16.90424512\n",
      "  16.77954978]\n",
      " ...\n",
      " [26.83513111 26.27179449 26.16101443 ... 16.31236921 16.87338631\n",
      "  16.74132307]\n",
      " [26.82404048 26.25446281 26.14994268 ... 16.30692625 16.86661873\n",
      "  16.73289937]\n",
      " [26.81532266 26.24281711 26.13940717 ... 16.30125824 16.85990699\n",
      "  16.72438165]]\n",
      "\n",
      "Corresponding labels for the test data:\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "Total number of samples in the dataset: 84420\n"
     ]
    }
   ],
   "source": [
    "data = load_data.read_data_sets()\n",
    "\n",
    "# get train data\n",
    "train_x = data.train.data\n",
    "\n",
    "# get train labels\n",
    "train_labels = data.train.labels\n",
    "\n",
    "# get test data\n",
    "test_x = data.test.data\n",
    "\n",
    "# get test labels\n",
    "test_labels = data.test.labels\n",
    "\n",
    "# get sample number\n",
    "n_samples = data.train.num_examples\n",
    "\n",
    "# Print the first 10 examples of training data and labels\n",
    "print(\"First 10 examples of training data:\")\n",
    "print(train_x[:10])\n",
    "print()\n",
    "\n",
    "print(\"Corresponding labels for the training data:\")\n",
    "print(train_labels[:10])\n",
    "print()\n",
    "\n",
    "# Print the first 10 examples of test data and labels\n",
    "print(\"First 10 examples of test data:\")\n",
    "print(test_x[:10])\n",
    "print()\n",
    "\n",
    "print(\"Corresponding labels for the test data:\")\n",
    "print(test_labels[:10])\n",
    "print()\n",
    "\n",
    "# Print the total number of samples in the dataset\n",
    "print(f\"Total number of samples in the dataset: {n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d501cc91-447f-44ad-a2b6-975cb0363c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label counts: {0: 28602, 1: 26628, 2: 29190}\n",
      "Test label counts: {0: 18438, 1: 19740, 2: 19950}\n"
     ]
    }
   ],
   "source": [
    "# Count the unique labels in the training set\n",
    "unique_train, counts_train = np.unique(train_labels, return_counts=True)\n",
    "label_counts_train = dict(zip(unique_train, counts_train))\n",
    "\n",
    "# Count the unique labels in the test set\n",
    "unique_test, counts_test = np.unique(test_labels, return_counts=True)\n",
    "label_counts_test = dict(zip(unique_test, counts_test))\n",
    "\n",
    "print(\"Training label counts:\", label_counts_train)\n",
    "print(\"Test label counts:\", label_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3cc329-bd7d-4637-a11b-8f4ac2720df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if dataset has missing values\n",
    "missing_rows_count = np.isnan(train_x).any(axis=1).sum()\n",
    "print(f\"Number of rows with missing values: {missing_rows_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efcf15d-6ddd-4359-94f1-f6342c5996d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label counts after balancing: {0: 29190, 1: 29190, 2: 29190}\n"
     ]
    }
   ],
   "source": [
    "# Balance the training data using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "balanced_train_x, balanced_train_labels = smote.fit_resample(train_x, train_labels)\n",
    "\n",
    "unique_train1, counts_train1 = np.unique(balanced_train_labels, return_counts=True)\n",
    "label_counts_train1 = dict(zip(unique_train1, counts_train1))\n",
    "\n",
    "print(\"Training label counts after balancing:\", label_counts_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d48f72a-d2cd-4993-9526-2915c124a543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 shuffled training labels\n",
      "[0 0 1 2 0 2 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle indices\n",
    "indices = np.arange(balanced_train_x.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Use shuffled indices to shuffle train_x and train_labels\n",
    "balanced_train_x_shuffled = balanced_train_x[indices]\n",
    "balanced_train_labels_shuffled = balanced_train_labels[indices]\n",
    "\n",
    "# Print the first 10 examples of shuffled training labels\n",
    "print(\"First 10 shuffled training labels\")\n",
    "print(balanced_train_labels_shuffled[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b866168-b563-4cac-b5e9-42afc8247150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "balanced_train_labels_shuffled_encoded = to_categorical(balanced_train_labels_shuffled, num_classes=3)\n",
    "test_labels_encoded = to_categorical(test_labels, num_classes=3)\n",
    "\n",
    "print(balanced_train_labels_shuffled_encoded[:10])\n",
    "print(test_labels_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09bd510-abe5-4427-a695-8a8e1a8b0056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 examples of training data:\n",
      "[[-1.56955821 -1.44485175 -1.29187886 ... -0.61953812 -0.58817583\n",
      "  -0.64833758]\n",
      " [ 1.18939037  1.72235096  1.34105149 ...  0.72793209  0.34534574\n",
      "   0.17853108]\n",
      " [ 0.74646566  0.44659276  0.54034448 ...  0.60687866 -0.10396648\n",
      "   0.06618927]\n",
      " ...\n",
      " [-0.88103756 -1.25887047 -1.35970375 ... -0.86265608 -0.81529286\n",
      "  -1.00670976]\n",
      " [-1.08720664 -0.70336561 -1.21834653 ... -0.9841089  -0.93170511\n",
      "   0.02157167]\n",
      " [ 0.24347842  0.04448774  0.32732339 ... -0.83747358 -1.07470651\n",
      "   5.99647619]]\n",
      "\n",
      "Corresponding labels for the training data:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "One-Hot encoded labels for the training data:\n",
      "[0 0 1 2 0 2 0 1 0 1]\n",
      "\n",
      "First 10 examples of test data:\n",
      "[[ 0.44138796 -0.00202854 -0.1746563  ... -0.41053994 -0.00116352\n",
      "  -0.22655811]\n",
      " [ 0.4407388  -0.00470152 -0.17757255 ... -0.40779565 -0.0018294\n",
      "  -0.22787341]\n",
      " [ 0.43632692 -0.01183294 -0.18535919 ... -0.40471564 -0.00188958\n",
      "  -0.22886742]\n",
      " ...\n",
      " [ 0.3945054  -0.07882315 -0.2471074  ... -0.42696935 -0.03418553\n",
      "  -0.26300493]\n",
      " [ 0.38445009 -0.09390231 -0.25759189 ... -0.4328514  -0.04126829\n",
      "  -0.27052752]\n",
      " [ 0.37654608 -0.10403446 -0.2675686  ... -0.43897667 -0.0482926\n",
      "  -0.27813409]]\n",
      "\n",
      "Corresponding labels for the test data:\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "One-Hot encoded labels for the test data:\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(balanced_train_x_shuffled)\n",
    "X_test = scaler.transform(test_x)\n",
    "\n",
    "y_train = balanced_train_labels_shuffled_encoded\n",
    "y_test = test_labels_encoded\n",
    "\n",
    "y_train1 = balanced_train_labels_shuffled\n",
    "y_test1 = test_labels\n",
    "\n",
    "print(\"First 10 examples of training data:\")\n",
    "print(X_train[:10])\n",
    "print()\n",
    "\n",
    "print(\"Corresponding labels for the training data:\")\n",
    "print(y_train[:10])\n",
    "print()\n",
    "\n",
    "print(\"One-Hot encoded labels for the training data:\")\n",
    "print(y_train1[:10])\n",
    "print()\n",
    "\n",
    "print(\"First 10 examples of test data:\")\n",
    "print(X_test[:10])\n",
    "print()\n",
    "\n",
    "print(\"Corresponding labels for the test data:\")\n",
    "print(y_test[:10])\n",
    "print()\n",
    "\n",
    "print(\"One-Hot encoded labels for the test data:\")\n",
    "print(y_test1[:10])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06b2d4-4211-4720-a5f4-ce12aed7d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define the parameter grid for logistic regression\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2','none'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    #'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 200, 500, 1000]\n",
    "}\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid_lr, cv=5, n_jobs=-1, scoring='accuracy', verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy of each model in the grid search\n",
    "print(\"Grid search results:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(f\"{mean:.4f} (+/-{std * 2:.4f}) for {params}\")\n",
    "\n",
    "# Print the best parameters found by the grid search\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator found by grid search to make predictions\n",
    "best_log_reg = grid_search.best_estimator_\n",
    "y_pred_log_reg = best_log_reg.predict(X_val)\n",
    "\n",
    "# Print classification report and accuracy\n",
    "print(\"Logistic Regression with Best Parameters:\")\n",
    "print(classification_report(y_val, y_pred_log_reg))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred_log_reg):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d5edb-08a2-4123-85f0-9aca0b3c9ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 100, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.2995%\n",
      "Saved the best model with accuracy: 71.2995%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 100, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 71.5886%\n",
      "Saved the best model with accuracy: 71.5886%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 71.9550%\n",
      "Saved the best model with accuracy: 71.9550%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 71.8948%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.8380%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 71.9498%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 71.9516%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.6355%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 100, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 70.4187%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 100, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 70.9434%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.2995%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 71.7073%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 71.9550%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 71.9567%\n",
      "Saved the best model with accuracy: 71.9567%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.8380%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 71.9498%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 71.9516%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 200, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 70.0471%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 200, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 70.3946%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.2995%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 71.7589%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 71.9550%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 71.9550%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.8380%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 71.9498%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 71.9516%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 500, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 69.4278%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 500, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 69.8373%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.2995%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 71.7589%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 71.9550%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 71.9550%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.8380%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 71.9498%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 71.9516%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 0.01, 'max_iter': 1000, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 69.2369%\n",
      "\n",
      "Training with params: {'C': 0.01, 'max_iter': 1000, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 69.4519%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 70.5804%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 71.1275%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 70.9572%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 71.1155%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.8260%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 71.0329%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 71.2342%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.6355%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 100, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 70.4187%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 100, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 70.9434%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 70.5804%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 70.9589%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 70.9572%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.8092%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.8260%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 70.9675%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 71.0587%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 200, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 70.0471%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 200, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 70.3946%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 70.5804%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 70.6028%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 70.9572%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.9555%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.8260%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 70.9641%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 70.9882%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 500, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 69.4278%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 500, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 69.8373%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 70.5804%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 70.7164%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 70.9572%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.9538%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.8260%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 70.9641%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 70.9882%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 0.1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 69.2369%\n",
      "\n",
      "Training with params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 69.4519%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 70.9417%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 70.9589%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 69.6583%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.7250%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.4217%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 70.4893%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 70.9572%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.6355%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 100, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 70.4187%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 100, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 70.9434%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 70.9417%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 70.4291%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 69.6583%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 69.9938%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.4217%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 70.1572%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 70.4875%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 200, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 70.0471%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 200, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 70.3946%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 70.9417%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 69.8269%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 69.6583%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 69.5706%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.4217%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 69.6463%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 69.9405%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 500, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 69.4278%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 500, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 69.8373%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy: 70.9417%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'sag'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'sag'} due to error: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Test accuracy: 69.3900%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Test accuracy: 69.6583%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Test accuracy: 69.6601%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy: 71.4217%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Test accuracy: 69.6085%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy: 69.6996%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'newton-cg'}\n",
      "Test accuracy: 68.1238%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'lbfgs'}\n",
      "Test accuracy: 70.5030%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'liblinear'}\n",
      "Skipping parameters {'C': 1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'liblinear'} due to error: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'sag'}\n",
      "Test accuracy: 69.2369%\n",
      "\n",
      "Training with params: {'C': 1, 'max_iter': 1000, 'penalty': 'none', 'solver': 'saga'}\n",
      "Test accuracy: 69.4519%\n",
      "\n",
      "Training with params: {'C': 10, 'max_iter': 100, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "Skipping parameters {'C': 10, 'max_iter': 100, 'penalty': 'l1', 'solver': 'newton-cg'} due to error: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 10, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "Skipping parameters {'C': 10, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'} due to error: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "Training with params: {'C': 10, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(43)\n",
    "random.seed(43)\n",
    "\n",
    "# Suppress convergence warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define grid search parameters\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2', 'none'],\n",
    "    'C': [0.01, 0.1, 1],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 200, 500, 1000]\n",
    "}\n",
    "\n",
    "# Initialize best accuracy and hyperparameters\n",
    "best_accuracy_lr = 0\n",
    "best_params_lr = {}\n",
    "best_model_path = 'Best_LR_model.pkl'\n",
    "\n",
    "# Loop over parameter combinations\n",
    "for params in ParameterGrid(param_grid_lr):\n",
    "    try:\n",
    "        print(f\"Training with params: {params}\")\n",
    "        \n",
    "        # Create the Logistic Regression model\n",
    "        model_LR = LogisticRegression(solver=params['solver'], penalty=params['penalty'], C=params['C'], max_iter=params['max_iter'], random_state=43)\n",
    "        \n",
    "        # Train the model\n",
    "        model_LR.fit(X_train, y_train1)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = model_LR.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test1, y_pred)\n",
    "        print(f\"Test accuracy: {accuracy:.4%}\")\n",
    "        \n",
    "        \n",
    "        # Check if this model is the best so far\n",
    "        if accuracy > best_accuracy_lr:\n",
    "            best_accuracy_lr = accuracy\n",
    "            best_params_lr = params\n",
    "            joblib.dump(model_LR, best_model_path)\n",
    "            print(f\"Saved the best model with accuracy: {accuracy:.4%}\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping parameters {params} due to error: {e}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Best accuracy of Logistic Regression: {best_accuracy_lr:.4%} with params: {best_params_lr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
